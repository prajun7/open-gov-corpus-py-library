{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07617628",
   "metadata": {},
   "source": [
    "# Dataset cleaning\n",
    "\n",
    "This notebook implements de-deduplication using the pandas library and advanced filtering using Cleanlab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e84220",
   "metadata": {},
   "source": [
    "#### Step 0: Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from cleanlab_tlm import TLM\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738862c",
   "metadata": {},
   "source": [
    "### Step 1: Filter EXACT duplicates\n",
    "\n",
    "This removes duplicates using pandas. Reduces the load for Cleanlab later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2594da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "INPUT_FILE = \"prompt_responses_normalized.csv\"\n",
    "OUTPUT_FILE = \"prompt_responses_deduplicated.csv\"\n",
    "COLUMN_TO_DEDUPLICATE = \"prompt\"\n",
    "\n",
    "# Loads a CSV, removes duplicate rows based on a specific column, and saves the result to a new file.\n",
    "def deduplicate_csv(filepath, column_name):\n",
    "    print(\"--- Starting Deduplication Script ---\")\n",
    "    \n",
    "    # 1. Check if the input file exists\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"FATAL ERROR: Input file not found at '{filepath}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 2. Load the CSV into a pandas DataFrame\n",
    "        print(f\"Loading data from '{filepath}'...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        original_row_count = len(df)\n",
    "        print(f\"Successfully loaded {original_row_count} rows.\")\n",
    "\n",
    "        # 3. Check if the specified column exists\n",
    "        if column_name not in df.columns:\n",
    "            print(f\"FATAL ERROR: Column '{column_name}' not found in the CSV file.\")\n",
    "            return\n",
    "\n",
    "        # 4. Perform the deduplication\n",
    "        print(f\"Removing duplicate rows based on the '{column_name}' column...\")\n",
    "        # - subset=[column_name]: Specifies that only this column should be checked for duplicates.\n",
    "        # - keep='first': Keeps the first occurrence of a duplicate and removes the rest.\n",
    "        deduplicated_df = df.drop_duplicates(subset=[column_name], keep='first')\n",
    "        new_row_count = len(deduplicated_df)\n",
    "        \n",
    "        # 5. Report the results\n",
    "        rows_removed = original_row_count - new_row_count\n",
    "        print(f\"\\n--- Results ---\")\n",
    "        print(f\"Original row count: {original_row_count}\")\n",
    "        print(f\"Rows removed: {rows_removed}\")\n",
    "        print(f\"Final row count: {new_row_count}\")\n",
    "        print(\"---------------\")\n",
    "\n",
    "        # 6. Save the cleaned DataFrame to a new CSV file\n",
    "        print(f\"Saving deduplicated data to '{OUTPUT_FILE}'...\")\n",
    "        deduplicated_df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(\"Script finished successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    deduplicate_csv(INPUT_FILE, COLUMN_TO_DEDUPLICATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573eb1aa",
   "metadata": {},
   "source": [
    "### Step 2: Adding Bias and Toxicity columns with Cleanlab Studios\n",
    "\n",
    "This script adds bias_score and toxic_score based on either the 'prompt' column or 'response column' to prompt_responses_deduplicated.csv using Cleanlab Studio’s Python API. It outputs the result as either response_with_bias_toxic.xsc or prompt_with_bias_toxic.csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters\n",
    "----------\n",
    "api_key          : Cleanlab Studio API key.\n",
    "dataset_name     : Name of the dataset already uploaded in Studio.\n",
    "text_column      : Column to analyze (\"prompt\" or \"response\").\n",
    "raw_csv_path     : Path to the original CSV on disk.\n",
    "output_csv_path  : Where to save the enriched CSV.\n",
    "project_name     : Studio project name.\n",
    "poll_interval    : Seconds between status polls.\n",
    "\"\"\"\n",
    "def add_bias_toxic_scores(\n",
    "    api_key: str,\n",
    "    dataset_name: str,\n",
    "    text_column: str,\n",
    "    raw_csv_path: str,\n",
    "    output_csv_path: str,\n",
    "    project_name: str = \"Bias & Toxicity scan\",\n",
    "    poll_interval: int = 60,\n",
    "):\n",
    "    studio = Studio(api_key)\n",
    "\n",
    "    # 1. Resolve dataset → ID\n",
    "    dataset_id = studio.poll_dataset_id_for_name(dataset_name)\n",
    "\n",
    "    # 2. Kick off the project\n",
    "    project_id = studio.create_project(\n",
    "        dataset_id=dataset_id,\n",
    "        project_name=project_name,\n",
    "        modality=\"text\",\n",
    "        task_type=\"unsupervised\",\n",
    "        model_type=\"regular\",\n",
    "        text_column=text_column,\n",
    "    )\n",
    "\n",
    "    # 3. Wait until a cleanset exists\n",
    "    while True:\n",
    "        try:\n",
    "            cleanset_id = studio.get_latest_cleanset_id(project_id)\n",
    "            break\n",
    "        except ValueError:\n",
    "            time.sleep(poll_interval)\n",
    "\n",
    "    studio.wait_until_cleanset_ready(cleanset_id)\n",
    "\n",
    "    # 4. Download scores\n",
    "    cl_cols = studio.download_cleanlab_columns(\n",
    "        cleanset_id, include_cleanlab_columns=True\n",
    "    )\n",
    "    scores_only = cl_cols[[\"cleanlab_row_ID\", \"bias_score\", \"toxic_score\"]]\n",
    "\n",
    "    # 5. Merge back to local CSV and save\n",
    "    raw = pd.read_csv(raw_csv_path)\n",
    "    merged = raw.merge(scores_only, left_index=True, right_on=\"cleanlab_row_ID\")\n",
    "    merged.to_csv(output_csv_path, index=False)\n",
    "    print(f\"✅  New file written: {output_csv_path}\")\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for \"prompt\" column\n",
    "add_bias_toxic_scores(\n",
    "    api_key= \"YOUR_API_KEY\",\n",
    "    dataset_name= \"prompt_responses_deduplicated.csv\",\n",
    "    text_column= \"prompt\",  \n",
    "    raw_csv_path= \"prompt_responses_deduplicated.csv\",\n",
    "    output_csv_path= \"prompt_with_bias_toxic.csv\",\n",
    ")\n",
    "\n",
    "# Run for \"response\" column\n",
    "add_bias_toxic_scores(\n",
    "    api_key= \"YOUR_API_KEY\",\n",
    "    dataset_name= \"prompt_responses_deduplicated.csv\",\n",
    "    text_column= \"response\",\n",
    "    raw_csv_path= \"response_responses_deduplicated.csv\",\n",
    "    output_csv_path= \"response_with_bias_toxic.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364fe0cf",
   "metadata": {},
   "source": [
    "### Step 3: Filter by bias_score and toxic_score\n",
    "\n",
    "Based on the scores, we can easily filter out low-quality, toxic, or unsafe content. A high score is a strong indicator of a problematic response. This cell does not actually remove rows or write to a new file, it just outputs the rows over either threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed92dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "INPUT_FILE = \"response_with_bias_toxic.csv\"  # The file from Cleanlab Studio with all the scores\n",
    "BIAS_THRESHOLD = 0.75\n",
    "TOXIC_THRESHOLD = 0.75\n",
    "\n",
    "def filter_by_scores(filepath, bias_thresh, toxic_thresh):\n",
    "    # 1. Check if the input file exists\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"FATAL ERROR: Input file not found at '{filepath}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 2. Load the CSV into a pandas DataFrame\n",
    "        print(f\"Loading data from '{filepath}'...\")\n",
    "        df = pd.read_csv(filepath)\n",
    "        original_row_count = len(df)\n",
    "        print(f\"Successfully loaded {original_row_count} rows.\")\n",
    "\n",
    "        # 3. Check if the required score columns exist\n",
    "        bias_col = 'cleanlab_bias_score'\n",
    "        toxic_col = 'cleanlab_toxic_score'\n",
    "        \n",
    "        if bias_col not in df.columns or toxic_col not in df.columns:\n",
    "            print(f\"FATAL ERROR: Required columns '{bias_col}' and/or '{toxic_col}' not found.\")\n",
    "            print(\"Please check the exact column names in your CSV file and update the script if needed.\")\n",
    "            return\n",
    "\n",
    "        # 4. Identify rows to be removed\n",
    "        condition_to_flag = (df[bias_col] >= bias_thresh) | (df[toxic_col] >= toxic_thresh)\n",
    "        response_rows_to_flag_df = df[condition_to_flag]\n",
    "        response_flagged_count = len(response_rows_to_flag_df)\n",
    "        \n",
    "        # 5. Report the results\n",
    "        print(f\"\\n--- Results ---\")\n",
    "        print(f\"Filtering with bias_score >= {bias_thresh} OR toxic_score >= {toxic_thresh}\")\n",
    "        print(f\"Num rows flagged: {len(response_rows_to_flag_df)}\")\n",
    "\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    flagged_rows = response_rows_to_flag_df[['prompt', 'response', \"cleanlab_row_ID\", bias_col, toxic_col]]\n",
    "\n",
    "    return flagged_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8161277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return rows with bias score > 0.75 or toxic_score > 0.75 based on the column 'response'\n",
    "response_flagged_rows = filter_by_scores(\"response_with_bias_toxic.csv\", BIAS_THRESHOLD, TOXIC_THRESHOLD)\n",
    "\n",
    "# Return rows with bias score > 0.75 or toxic_score > 0.75 based on the column 'prompt'\n",
    "prompt_flagged_rows = filter_by_scores(\"prompt_with_bias_toxic.csv\" , BIAS_THRESHOLD, TOXIC_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644324a3",
   "metadata": {},
   "source": [
    "### Step 4: Combine the two dataframes into one\n",
    "\n",
    "Combine the rows flagged with high toxic_score and bias_score from the Cleanlab analysis based on the \"prompt\" column and from the Cleanlab analysis based on the \"response\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e330b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag each DataFrame’s score columns\n",
    "prompt_df   = prompt_flagged_rows.rename(\n",
    "    columns={\n",
    "        \"cleanlab_bias_score\":   \"prompt_bias_score\",\n",
    "        \"cleanlab_toxic_score\":  \"prompt_toxic_score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response_df = response_flagged_rows.rename(\n",
    "    columns={\n",
    "        \"cleanlab_bias_score\":   \"response_bias_score\",\n",
    "        \"cleanlab_toxic_score\":  \"response_toxic_score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Merge on the row identifier\n",
    "combined_flagged_rows = (\n",
    "    pd.merge(prompt_df, response_df,\n",
    "             on=\"cleanlab_row_ID\",\n",
    "             how=\"outer\",\n",
    "             suffixes=(\"_prompt\", \"_response\"))\n",
    "      .sort_values(\"cleanlab_row_ID\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Fill missing prompt/response values\n",
    "combined_flagged_rows[\"prompt\"] = combined_flagged_rows[\"prompt_prompt\"].combine_first(combined_flagged_rows[\"prompt_response\"])\n",
    "combined_flagged_rows[\"response\"] = combined_flagged_rows[\"response_response\"].combine_first(combined_flagged_rows[\"response_prompt\"])\n",
    "\n",
    "# Replace NaNs with 0 for easier filtering\n",
    "combined_flagged_rows.fillna(\n",
    "    {\"prompt_bias_score\":   0, \"prompt_toxic_score\":   0,\n",
    "     \"response_bias_score\": 0, \"response_toxic_score\": 0},\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Drop redundant columns\n",
    "combined_flagged_rows = combined_flagged_rows[[\n",
    "    \"prompt\", \"response\", \"cleanlab_row_ID\",\n",
    "    \"prompt_bias_score\", \"prompt_toxic_score\",\n",
    "    \"response_bias_score\", \"response_toxic_score\"\n",
    "]]\n",
    "\n",
    "# display(combined_flagged_rows)\n",
    "\n",
    "# Export to CSV\n",
    "combined_flagged_rows.to_csv(\"flagged_bias_toxic_rows.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6780dcfa",
   "metadata": {},
   "source": [
    "### Step 5: Remove near-duplicate columns\n",
    "\n",
    "The final labelled dataset from Cleanlabs (analyzed on 'prompt') was saved (10,000 rows) with additional columns. They are:\n",
    "\n",
    "- PII\n",
    "- Toxic Score\n",
    "- Bias Score\n",
    "- Near Duplicate Cluster ID\n",
    "\n",
    "This script removes near‑duplicates and keeps only one row per cluster ID.\n",
    "\n",
    "- Near Duplicate Cluster is NaN for unique rows; non‑zero for rows that belong to a duplicate cluster.  \n",
    "- For every cluster ID > 0, we keep the first row encountered and drop the rest.  \n",
    "- Finally, we drop the Near Duplicate Cluster ID column in the final file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e62066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "INPUT_FILE   = \"cleanlab_PROMPT-Bias-&-Toxicity-scan_2025-07-25-14-52-52.csv\"\n",
    "OUTPUT_FILE  = \"cleaned_prompt_response.csv\"\n",
    "CLUSTER_COL  = \"cleanlab_near_duplicate_cluster_id\"\n",
    "\n",
    "# 1) Load the CSV\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# 2) Make sure the cluster column is numeric (NaN for blanks)\n",
    "cluster = pd.to_numeric(df[CLUSTER_COL], errors=\"coerce\")\n",
    "\n",
    "# 3) Identify extra duplicates:\n",
    "#    - rows whose cluster ID is not‑NaN\n",
    "#    - AND are *not* the first occurrence of that cluster\n",
    "extra_dups = (cluster.notna() &\n",
    "              df.duplicated(subset=CLUSTER_COL, keep=\"first\"))\n",
    "\n",
    "# 4) Drop the extras and the cluster column\n",
    "df_clean = (df[~extra_dups]\n",
    "            .drop(columns=[CLUSTER_COL])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "# 5) Save\n",
    "df_clean.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"{OUTPUT_FILE} written. {extra_dups.sum()} rows removed.\")\n",
    "print(f\"Final row count: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dfe8c8",
   "metadata": {},
   "source": [
    "### Step 6: Repeat everything for the 246 rows that weren't added to Cleanlab because of the 10,000 row limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee552a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset for the remaining 246 rows.\n",
    "df = pd.read_csv(\"prompt_responses_deduplicated.csv\")\n",
    "last_246 = df.tail(246)\n",
    "\n",
    "# display(last_246.head())\n",
    "\n",
    "last_246.to_csv(\"last_246_df.csv\", index=False)\n",
    "\n",
    "# Upload to Cleanlab before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e861a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanlab analysis on \"response\" column\n",
    "add_bias_toxic_scores(\n",
    "    api_key= \"YOUR_API_KEY\",\n",
    "    dataset_name= \"prompt_responses_deduplicated.csv\",\n",
    "    text_column= \"response\",\n",
    "    raw_csv_path= \"response_responses_deduplicated.csv\",\n",
    "    output_csv_path= \"246_response_with_bias_toxic.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e3fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanlab analysis on \"prompt\" column\n",
    "add_bias_toxic_scores(\n",
    "    api_key= \"YOUR_API_KEY\",\n",
    "    dataset_name= \"last_246_df.csv\",\n",
    "    text_column= \"prompt\",\n",
    "    raw_csv_path= \"last_246_df.csv\",\n",
    "    output_csv_path= \"246_prompt_with_bias_toxic.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b1d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by threshold\n",
    "flagged_prompt_rows_246 = filter_by_scores(\"246_prompt_with_bias_toxic.csv\", BIAS_THRESHOLD, TOXIC_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bf3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged_response_rows_246 = filter_by_scores(\"246_response_with_bias_toxic.csv\", BIAS_THRESHOLD, TOXIC_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31074036",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(flagged_response_rows_246)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two dataframes\n",
    "\n",
    "# Tag each DataFrame’s score columns\n",
    "prompt_df   = flagged_prompt_rows_246.rename(\n",
    "    columns={\n",
    "        \"cleanlab_bias_score\":   \"prompt_bias_score\",\n",
    "        \"cleanlab_toxic_score\":  \"prompt_toxic_score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response_df = flagged_response_rows_246.rename(\n",
    "    columns={\n",
    "        \"cleanlab_bias_score\":   \"response_bias_score\",\n",
    "        \"cleanlab_toxic_score\":  \"response_toxic_score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Merge on the row identifier\n",
    "combined_flagged_rows_246 = (\n",
    "    pd.merge(prompt_df, response_df,\n",
    "             on=\"cleanlab_row_ID\",      # common key\n",
    "             how=\"outer\")               # union of both sets\n",
    "      .sort_values(\"cleanlab_row_ID\")\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Replace NaNs with 0 for easier filtering\n",
    "combined_flagged_rows_246.fillna(\n",
    "    {\"prompt_bias_score\":   0, \"prompt_toxic_score\":   0,\n",
    "     \"response_bias_score\": 0, \"response_toxic_score\": 0},\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "display(combined_flagged_rows_246[['prompt_x', 'response_y', 'prompt_bias_score', 'prompt_toxic_score', 'response_bias_score', 'response_toxic_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd33193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append combined_flagged_rows_246 to flagged_bias_toxic_rows.csv, removing duplicates\n",
    "master_csv = \"flagged_bias_toxic_rows.csv\"\n",
    "\n",
    "try:\n",
    "    master_df = pd.read_csv(master_csv)\n",
    "except FileNotFoundError:\n",
    "    master_df = pd.DataFrame(columns=combined_flagged_rows_246.columns)\n",
    "\n",
    "# Concatenate and deduplicate\n",
    "updated_df = pd.concat([master_df, combined_flagged_rows_246], ignore_index=True)\n",
    "updated_df = updated_df.drop_duplicates(subset=[\"cleanlab_row_ID\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# Drop the column cleanlab_row_ID because it's not needed anymore\n",
    "updated_df = updated_df.drop(columns=[\"cleanlab_row_ID\"])\n",
    "\n",
    "# Save back to disk\n",
    "updated_df.to_csv(master_csv, index=False)\n",
    "print(f\"flagged_bias_toxic_rows.csv updated. Total rows: {len(updated_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e5f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near deduplication for last_246_df.csv\n",
    "\n",
    "# Config\n",
    "INPUT_FILE   = \"cleanlab_246_prompt_with_bias_toxic_2025-07-25-19-42-47.csv\"\n",
    "OUTPUT_FILE  = \"246_cleaned_prompt_response.csv\"\n",
    "CLUSTER_COL  = \"cleanlab_near_duplicate_cluster_id\"\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Make sure the cluster column is numeric (NaN for blanks)\n",
    "cluster = pd.to_numeric(df[CLUSTER_COL], errors=\"coerce\")\n",
    "\n",
    "# Identify extra duplicates:\n",
    "#    - rows whose cluster ID is not‑NaN\n",
    "#    - AND are *not* the first occurrence of that cluster\n",
    "extra_dups = (cluster.notna() &\n",
    "              df.duplicated(subset=CLUSTER_COL, keep=\"first\"))\n",
    "\n",
    "\n",
    "# Drop the extras and the cluster column\n",
    "df_clean = (df[~extra_dups]\n",
    "            .drop(columns=[CLUSTER_COL])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "# Save\n",
    "df_clean.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"{OUTPUT_FILE} written. {extra_dups.sum()} rows removed.\")\n",
    "print(f\"Final row count: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 246_cleaned_prompt_response.csv to the end of cleaned_prompt_response.csv\n",
    "df_clean_246 = pd.read_csv(\"246_cleaned_prompt_response.csv\")\n",
    "df_clean = pd.read_csv(\"cleaned_prompt_response.csv\")\n",
    "\n",
    "df_combined = pd.concat([df_clean_246, df_clean], ignore_index=True).drop(columns=['cleanlab_row_ID']).reset_index(drop=True)\n",
    "# Display the combined DataFrame\n",
    "# display(df_combined.head())\n",
    "\n",
    "df_combined.to_csv(\"dataset_final.csv\", index=False)\n",
    "\n",
    "print(len(df_combined))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
