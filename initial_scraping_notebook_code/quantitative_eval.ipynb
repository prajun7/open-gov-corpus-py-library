{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e00aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import evaluate # Hugging Face's library for metrics\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df053be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models for scoring...\n",
      "Models initialized.\n",
      "\n",
      "--- Starting Quantitative Analysis Script ---\n",
      "Loading benchmark results from 'benchmark_results.csv'...\n",
      "Found 6 model(s) to evaluate.\n",
      "\n",
      "--- Scoring model: Answer_meta-llama_llama-3.1-8b-instruct ---\n",
      "  - Calculating F1 and Cosine Similarity scores...\n",
      "  - Calculating BERTScore (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Scoring for Answer_meta-llama_llama-3.1-8b-instruct complete.\n",
      "\n",
      "--- Scoring model: Answer_meta-llama_llama-3.1-70b-instruct ---\n",
      "  - Calculating F1 and Cosine Similarity scores...\n",
      "  - Calculating BERTScore (this may take a moment)...\n",
      "  - Scoring for Answer_meta-llama_llama-3.1-70b-instruct complete.\n",
      "\n",
      "--- Scoring model: Answer_mistralai_mistral-7b-instruct-v0.3 ---\n",
      "  - Calculating F1 and Cosine Similarity scores...\n",
      "  - Calculating BERTScore (this may take a moment)...\n",
      "  - Scoring for Answer_mistralai_mistral-7b-instruct-v0.3 complete.\n",
      "\n",
      "--- Scoring model: Answer_mistralai_mixtral-8x22b-instruct ---\n",
      "  - Calculating F1 and Cosine Similarity scores...\n",
      "  - Calculating BERTScore (this may take a moment)...\n",
      "  - Scoring for Answer_mistralai_mixtral-8x22b-instruct complete.\n",
      "\n",
      "--- Scoring model: Answer_qwen_qwen-2.5-7b-instruct ---\n",
      "  - Calculating F1 and Cosine Similarity scores...\n",
      "  - Calculating BERTScore (this may take a moment)...\n",
      "  - Scoring for Answer_qwen_qwen-2.5-7b-instruct complete.\n",
      "\n",
      "--- Scoring model: Answer_qwen_qwen-2.5-72b-instruct ---\n",
      "  - Calculating F1 and Cosine Similarity scores...\n",
      "  - Calculating BERTScore (this may take a moment)...\n",
      "  - Scoring for Answer_qwen_qwen-2.5-72b-instruct complete.\n",
      "\n",
      "--- Average Performance Scores ---\n",
      "                             Model Avg F1 Score Avg Cosine Similarity Avg BERTScore\n",
      "  meta-llama_llama-3.1-8b-instruct       0.2604                0.7171        0.8656\n",
      " meta-llama_llama-3.1-70b-instruct       0.3389                0.7587        0.8825\n",
      "mistralai_mistral-7b-instruct-v0.3       0.2444                0.7297        0.8673\n",
      "  mistralai_mixtral-8x22b-instruct       0.2854                0.7502        0.8764\n",
      "         qwen_qwen-2.5-7b-instruct       0.3187                0.7646        0.8840\n",
      "        qwen_qwen-2.5-72b-instruct       0.3386                0.7792        0.8904\n",
      "------------------------------------\n",
      "\n",
      "Saving detailed scores to 'benchmark_scores.csv'...\n",
      "Script finished successfully.\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "INPUT_FILE = \"benchmark_results.csv\"\n",
    "OUTPUT_FILE = \"benchmark_scores.csv\"\n",
    "GROUND_TRUTH_COLUMN = \"Answer\" \n",
    "\n",
    "# Initialize models\n",
    "print(\"Initializing models for scoring...\")\n",
    "# Load a model for calculating sentence embeddings (for cosine similarity)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# Load the BERTScore metric from the evaluate library\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "print(\"Models initialized.\")\n",
    "\n",
    "# Calculates the F1 score between two strings.\n",
    "def calculate_f1(prediction, reference):\n",
    "    pred_tokens = prediction.lower().split()\n",
    "    ref_tokens = reference.lower().split()\n",
    "    \n",
    "    # Create a vocabulary of all unique tokens\n",
    "    vocab = sorted(list(set(pred_tokens + ref_tokens)))\n",
    "    \n",
    "    # Create binary vectors\n",
    "    pred_vec = [1 if token in pred_tokens else 0 for token in vocab]\n",
    "    ref_vec = [1 if token in ref_tokens else 0 for token in vocab]\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    return f1_score(ref_vec, pred_vec, zero_division=0)\n",
    "\n",
    "# Calculates the cosine similarity between two strings.\n",
    "def calculate_cosine_similarity(prediction, reference):\n",
    "    # Encode both sentences into vector embeddings\n",
    "    emb1 = embedding_model.encode(prediction, convert_to_tensor=True)\n",
    "    emb2 = embedding_model.encode(reference, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cos_sim = util.pytorch_cos_sim(emb1, emb2)\n",
    "    return cos_sim.item()\n",
    "\n",
    "# Calculates BERTScore for a batch of predictions and references.\n",
    "def calculate_bertscore(predictions, references):\n",
    "    # Note: BERTScore is best run in batches for efficiency\n",
    "    results = bertscore.compute(\n",
    "        predictions=predictions, \n",
    "        references=references, \n",
    "        lang=\"en\",\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu' # Use GPU if available\n",
    "    )\n",
    "    # Return the F1 measure from BERTScore\n",
    "    return results['f1']\n",
    "\n",
    "# Main function to run the quantitative analysis.\n",
    "def main():\n",
    "    print(\"\\n--- Starting Quantitative Analysis Script ---\")\n",
    "\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"FATAL ERROR: Input file not found at '{INPUT_FILE}'\")\n",
    "        return\n",
    "\n",
    "    # Load the benchmark results\n",
    "    print(f\"Loading benchmark results from '{INPUT_FILE}'...\")\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # Add the ground truth column from the clean dataset (it was not included in the benchmarking script, whoops)\n",
    "    try:\n",
    "        ground_truth_df = pd.read_csv(\"dataset_final.csv\")\n",
    "        n_rows = (ground_truth_df.shape[0])\n",
    "        df[GROUND_TRUTH_COLUMN] = ground_truth_df['response'].head(n_rows)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL ERROR: Ground truth file {ground_truth_df} not found.\")\n",
    "        return\n",
    "    except KeyError:\n",
    "        print(f\"FATAL ERROR: 'response' column not found in ground truth file.\")\n",
    "        return\n",
    "\n",
    "    # Reorder columns to place ground truth answer after the question for clarity (index = 1)\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(1, cols.pop(cols.index(GROUND_TRUTH_COLUMN)))\n",
    "    df = df[cols]\n",
    "\n",
    "    # Identify the model answer columns\n",
    "    model_columns = [col for col in df.columns if col.startswith('Answer_')]\n",
    "    if not model_columns:\n",
    "        print(\"FATAL ERROR: No model answer columns (starting with 'Answer_') found in the input file.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Found {len(model_columns)} model(s) to evaluate.\")\n",
    "\n",
    "    # Calculate scores for each model\n",
    "    for model_col in model_columns:\n",
    "        print(f\"\\n--- Scoring model: {model_col} ---\")\n",
    "        \n",
    "        # Ensure data is in string format and handle missing values\n",
    "        predictions = df[model_col].astype(str).fillna('').tolist()\n",
    "        references = df[GROUND_TRUTH_COLUMN].astype(str).fillna('').tolist()\n",
    "\n",
    "        # Calculate F1 and Cosine Similarity row-by-row\n",
    "        print(\"  - Calculating F1 and Cosine Similarity scores...\")\n",
    "        df[f'{model_col}_F1'] = [calculate_f1(p, r) for p, r in zip(predictions, references)]\n",
    "        df[f'{model_col}_CosineSim'] = [calculate_cosine_similarity(p, r) for p, r in zip(predictions, references)]\n",
    "\n",
    "        # Calculate BERTScore in a batch for speed\n",
    "        print(\"  - Calculating BERTScore (this may take a moment)...\")\n",
    "        df[f'{model_col}_BERTScore'] = calculate_bertscore(predictions, references)\n",
    "        \n",
    "        print(f\"  - Scoring for {model_col} complete.\")\n",
    "\n",
    "    # Calculate and display average scores\n",
    "    print(\"\\n--- Average Performance Scores ---\")\n",
    "    summary = []\n",
    "    for model_col in model_columns:\n",
    "        model_name = model_col.replace('Answer_', '')\n",
    "        avg_f1 = df[f'{model_col}_F1'].mean()\n",
    "        avg_cosine = df[f'{model_col}_CosineSim'].mean()\n",
    "        avg_bert = df[f'{model_col}_BERTScore'].mean()\n",
    "        summary.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Avg F1 Score\": f\"{avg_f1:.4f}\",\n",
    "            \"Avg Cosine Similarity\": f\"{avg_cosine:.4f}\",\n",
    "            \"Avg BERTScore\": f\"{avg_bert:.4f}\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "    # 4. Save the detailed scores to a new CSV\n",
    "    print(f\"\\nSaving detailed scores to '{OUTPUT_FILE}'...\")\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(\"Script finished successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
